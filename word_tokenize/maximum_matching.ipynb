{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "f = open(\"word_tokenize/dictionary.txt\", mode=\"r\", encoding=\"utf-8\").read().splitlines()\n",
    "\n",
    "dictionary = set(f)\n",
    "\n",
    "train = open(\"word_tokenize/train_data.txt\", mode=\"r\", encoding=\"utf-8\").read().splitlines()\n",
    "for item in train:\n",
    "    item = item.strip().split()\n",
    "    if \"\" in item:\n",
    "        item.remove(\"\")\n",
    "    for word in item:\n",
    "        word = word.replace(\"_\", \" \")\n",
    "        dictionary.add(word)\n",
    "# special_character\n",
    "special_character = [\",\", \"–\", \";\", \":\", '''\"''', \"(\", \")\", \"?\", \"!\", \"...\"]\n",
    "special_character = set(special_character)\n",
    "\n",
    "\n",
    "# split sentences to words with space and special character\n",
    "def split_sentences(sentences):\n",
    "    final_array = []\n",
    "    cur = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        # if has special character -> split\n",
    "        if sentences[i] in special_character:\n",
    "            # reported speech\n",
    "            if sentences[i] == \":\":\n",
    "                if sentences[i + 1] == \" \":\n",
    "                    sentences = sentences[:i + 1] + sentences[i + 2:]\n",
    "            # check ! and ? at the end of sentences\n",
    "            if not (sentences[i] == \"!\" and i != len(sentences) - 1):\n",
    "                left = sentences[cur:i].strip().split(\" \")\n",
    "\n",
    "                if left[0] != \"\":\n",
    "\n",
    "                    for item in left:\n",
    "                        final_array.append(item)\n",
    "                final_array.append(sentences[i])\n",
    "                cur = i + 1\n",
    "        i = i + 1\n",
    "    # if no special character -> split\n",
    "    if cur < len(sentences):\n",
    "        left = sentences[cur:i].strip().split(\" \")\n",
    "        for item in left:\n",
    "            final_array.append(item)\n",
    "    return final_array\n",
    "\n",
    "\n",
    "# match word with maximum matching\n",
    "def split_string(string):\n",
    "    array = split_sentences(string)\n",
    "    i = 0\n",
    "    pos = 4\n",
    "    final_arr = []\n",
    "    while i < len(array):\n",
    "        while True:\n",
    "            cur_string = \" \".join(array[i:i + pos])\n",
    "            # if pos = 1 -> break\n",
    "            if pos == 1:\n",
    "                break\n",
    "            # if string has\n",
    "            if not re.search(\"[,()–:;.?!\\\"]\", cur_string):\n",
    "                # check noun, word start with capital letter and not contain number\n",
    "                if cur_string.istitle() and not re.search(\"\\d\", cur_string):\n",
    "                    # check if word is at the beginning of the line. Or after ? or !\n",
    "                    if i == 0:\n",
    "                        # if word is at the beginning of the line -> break\n",
    "                        if array[i].lower() in dictionary:\n",
    "                            final_arr.append(array[i])\n",
    "                            cur_string = \" \".join(array[i + 1:i + pos])\n",
    "\n",
    "                    i = i + pos - 1\n",
    "                    break\n",
    "                # check word is in dictionary\n",
    "                if cur_string.lower() in dictionary:\n",
    "                    i = i + pos - 1\n",
    "                    break\n",
    "            pos = pos - 1\n",
    "        final_arr.append(cur_string.replace(\" \", \"_\"))\n",
    "        pos = 4\n",
    "        i = i + 1\n",
    "    return final_arr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open(\"word_tokenize/test_data.txt\", mode=\"r\", encoding=\"utf-8\").read().splitlines()\n",
    "for item in sentences:\n",
    "    result = split_string(item)\n",
    "    print(*result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
